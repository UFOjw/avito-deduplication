{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61f0f5da-d12d-4c52-8352-414ab4bf28f1",
   "metadata": {},
   "source": [
    "# Training pair of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fe9b7a54-4494-4231-b15f-2bf1e73e4886",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import nn, amp\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import json\n",
    "from transformers import (\n",
    "    AutoModel,\n",
    "    AutoTokenizer,\n",
    "    get_scheduler,\n",
    ")\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "499741f9-b7b8-49fd-8ec0-eaf0b6783b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"avitotech_data\\\\avitotech_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a96f432a-2e4c-4d4c-957f-bbebb094b5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_triplets(data):\n",
    "    triplets = []\n",
    "\n",
    "    for anchor, info in data.items():\n",
    "        pair = info['pair']\n",
    "        is_double = info['is_double']\n",
    "\n",
    "        positives = [p for p, d in zip(pair, is_double) if d == 1]\n",
    "        negatives = [p for p, d in zip(pair, is_double) if d == 0]\n",
    "\n",
    "        assert len(negatives) >= 6 * len(positives), f\"Not enough negatives for anchor {anchor}\"\n",
    "\n",
    "        for i, pos in enumerate(positives):\n",
    "            start = i * 6\n",
    "            end = start + 6\n",
    "            negs_for_pos = negatives[start:end]\n",
    "            for neg in negs_for_pos:\n",
    "                triplets.append((anchor, pos, neg))\n",
    "\n",
    "    return triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ca55d79-3623-4094-bcab-c6055ba05cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_texts_in_dict(data: dict, tokenizer, max_length: int = 384):\n",
    "    for key, text in tqdm(data.items()):\n",
    "        if 'input_ids' in text:\n",
    "            continue\n",
    "        tokens = tokenizer(\n",
    "            text,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        tokens.pop(\"token_type_ids\", None)\n",
    "        # Убираем batch размерность (1, seq_len) -> (seq_len,)\n",
    "        data[key] = {\n",
    "            k: v.squeeze(0) for k, v in tokens.items()\n",
    "        }\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3d21f4-d5cb-4f6f-b8ad-02b8ffe352c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_numbers(filename):\n",
    "    # Получаем числа из строки\n",
    "    parts = filename.replace(\".pt\", \"\").split(\"_\")\n",
    "    return int(parts[2]), int(parts[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ea022e7-26a3-42a4-bb4a-a19234de1fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnPooling(nn.Module):\n",
    "    \"\"\"\n",
    "    Learnable attention pooling: весит каждый токен по-разному.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size: int = 312):\n",
    "        super().__init__()\n",
    "        # \"Запрос\" q, который обучается вместе с остальной сеткой\n",
    "        self.q = nn.Parameter(torch.randn(hidden_size))\n",
    "\n",
    "    def forward(self,\n",
    "                hidden_states: torch.Tensor,\n",
    "                attention_mask: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        hidden_states : (B, T, 312)\n",
    "        attention_mask: (B, T)\n",
    "        \"\"\"\n",
    "        # (B,T) — скалярное произведение q и каждого токена\n",
    "        scores = (hidden_states @ self.q) / hidden_states.size(-1) ** 0.5\n",
    "        scores = scores.masked_fill(attention_mask == 0, -1e4)  # exclude pads\n",
    "        weights = F.softmax(scores, dim=1).unsqueeze(-1)        # (B,T,1)\n",
    "\n",
    "        pooled = (weights * hidden_states).sum(dim=1)           # (B,312)\n",
    "        return pooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09666606-c15b-4d4a-a5c1-7344554ef7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairEncoder(nn.Module):\n",
    "    def __init__(self, d=512):\n",
    "        super().__init__()\n",
    "        self.proj_img = nn.Sequential(nn.Linear(768, d), nn.GELU(), nn.LayerNorm(d))\n",
    "        self.proj_txt = nn.Sequential(nn.Linear(312, d), nn.GELU(), nn.LayerNorm(d))\n",
    "        self.fuse     = nn.Sequential(\n",
    "            nn.Linear(2*d, 2*d), nn.GELU(),\n",
    "            nn.Linear(2*d, d),   nn.LayerNorm(d)\n",
    "        )\n",
    "\n",
    "    def forward(self, v_img, v_txt):\n",
    "        z_i = F.normalize(self.proj_img(v_img), dim=-1)\n",
    "        z_t = F.normalize(self.proj_txt(v_txt), dim=-1)\n",
    "        pair = torch.cat([z_i, z_t], dim=-1)\n",
    "        return F.normalize(self.fuse(pair), dim=-1)   # (B,d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64de141d-7c14-4d71-b08d-3bd7f560c95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(model, attn_pool, encoder, input_ids, attention_mask, image_embed):\n",
    "    out = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    pooled = attn_pool(out.last_hidden_state.float(), attention_mask)\n",
    "    return encoder(image_embed, pooled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf94cc3b-c9d7-4ed9-ab01-ad790713f847",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletDataset(Dataset):\n",
    "    def __init__(self, triplets, tokenized_data_text, tokenized_data_img):\n",
    "        self.triplets = triplets\n",
    "        self.tokenized_data_img = tokenized_data_img\n",
    "        self.tokenized_data_text = tokenized_data_text\n",
    "\n",
    "        self.keys_img = set(tokenized_data_img.keys())\n",
    "        self.keys_text = set(tokenized_data_text.keys())\n",
    "        self.text_empty_vector = torch.zeros(512)\n",
    "        self.img_empty_vector = torch.zeros(768)\n",
    "\n",
    "    def get_img(self, key):\n",
    "        return self.tokenized_data_img.get(key, self.text_empty_vector)\n",
    "\n",
    "    def get_text(self, key):\n",
    "        return self.tokenized_data_text.get(key, {\n",
    "            'input_ids': self.text_empty_vector,\n",
    "            'attention_mask': self.text_empty_vector\n",
    "        })\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        a, p, n = self.triplets[idx]\n",
    "        img_a = self.get_img(a)\n",
    "        img_p = self.get_img(p)\n",
    "        img_n = self.get_img(n)\n",
    "\n",
    "        text_a = self.get_text(a)\n",
    "        text_p = self.get_text(p)\n",
    "        text_n = self.get_text(n)\n",
    "        return {\n",
    "            'anchor_input_ids':        text_a['input_ids'].squeeze(0),\n",
    "            'anchor_attention_mask':   text_a['attention_mask'].squeeze(0),\n",
    "            'anchor_encoded_image':    img_a.squeeze(0),\n",
    "            'positive_input_ids':      text_p['input_ids'].squeeze(0),\n",
    "            'positive_attention_mask': text_p['attention_mask'].squeeze(0),\n",
    "            'positive_encoded_image':  img_p.squeeze(0),\n",
    "            'negative_input_ids':      text_n['input_ids'].squeeze(0),\n",
    "            'negative_attention_mask': text_n['attention_mask'].squeeze(0),\n",
    "            'negative_encoded_image':  img_n.squeeze(0),\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.triplets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a352f08-1cb2-440f-b86b-d645e4bfd777",
   "metadata": {},
   "source": [
    "## Loading image emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065dd02d-d91d-4b87-b0e6-eda1a6030a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'unzipped\\\\train_images_embeddings_merged.pt'\n",
    "tokenized_data_img = torch.load(PATH, map_location=\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df90f3d-462f-4f0d-a5a2-962f4811c293",
   "metadata": {},
   "source": [
    "## Loading trained text model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3395dc78-cdd7-4a41-82c6-5d8ac47e4a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = glob.glob('trained_text_models\\\\lora_triplet_*_step_*.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cb616b-19b4-42c1-b1f7-286f11e9a395",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_name = \"cointegrated/rubert-tiny2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,  # размер low-rank матриц A и B\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"query\", \"value\"],  # модули attention, которые дообучаем\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"FEATURE_EXTRACTION\"\n",
    ")\n",
    "\n",
    "model_lora = get_peft_model(model, lora_config)\n",
    "attn_pool = AttnPooling(312)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae1c9c8-c200-4fff-ad7e-c0bf0aac9681",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_chpt = max(models, key=extract_numbers)\n",
    "checkpoint = torch.load(last_chpt, map_location=device)\n",
    "\n",
    "model_lora.load_state_dict(checkpoint['model_state_dict'])\n",
    "attn_pool.load_state_dict(checkpoint['attn_state_dict'])\n",
    "model_lora = model_lora.to(device)\n",
    "attn_pool = attn_pool.to(device)\n",
    "model_lora.eval()\n",
    "attn_pool.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece19784-45a1-4d5e-8bcf-812795e4e858",
   "metadata": {},
   "source": [
    "### Freeze model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecb04cf-43cf-43b6-9075-46e8effde1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model_lora.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in attn_pool.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d35904-b30c-4b8d-a186-1659fdb647ae",
   "metadata": {},
   "source": [
    "## Loading train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18524a13-65a9-47aa-977d-54700baba02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('to_undergo.json', 'r') as f:\n",
    "    ids_trunc = json.load(f)\n",
    "\n",
    "with open(\"cards_train.json\", \"r\") as file:\n",
    "    cards_train = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b18f1b-bb0e-478d-86e9-6176c2b82156",
   "metadata": {},
   "outputs": [],
   "source": [
    "triplets = build_triplets(ids_trunc)\n",
    "train_triplets, val_triplets = train_test_split(triplets, test_size=0.1, random_state=42)\n",
    "\n",
    "tokenized_data_text = tokenize_texts_in_dict(cards_train, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe6e7f8-d9c3-4887-9dfc-66e6da1c8688",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TripletDataset(train_triplets, tokenized_data_text, tokenized_data_img)\n",
    "val_dataset   = TripletDataset(val_triplets, tokenized_data_text, tokenized_data_img)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02427fa0-7d11-4e73-8b4e-85058a797211",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046cfa56-cc7a-42a8-a4af-6b0c2e77ea98",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = PairEncoder(512)\n",
    "optimizer = torch.optim.AdamW(\n",
    "    filter(lambda p: p.requires_grad, encoder.parameters()),\n",
    "    lr=2e-5\n",
    ")\n",
    "criterion = nn.TripletMarginLoss(margin=2, p=2)\n",
    "\n",
    "epochs = 3\n",
    "gradient_accumulation_steps = 5\n",
    "num_training_steps = epochs * len(train_loader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "scaler = amp.GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc6ee5b-4b07-4109-bbfd-3727a0fdda5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"trained_encoder\", exist_ok=True)\n",
    "\n",
    "os.chdir(\"trained_encoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb584a54-e6ee-4e85-a8bd-617b22b08c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = glob.glob('encoder_*_step_*.pt')\n",
    "\n",
    "if models:\n",
    "    last_chpt = max(models, key=extract_numbers)\n",
    "    checkpoint = torch.load(last_chpt, map_location=device)\n",
    "    \n",
    "    encoder.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    scaler.load_state_dict(checkpoint['scaler_state_dict'])\n",
    "\n",
    "encoder = encoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc2fda1-0261-40a2-8cac-93203bf73a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "    encoder.train()\n",
    "    total_loss = 0.0\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch}\")\n",
    "\n",
    "    cnt = len(train_loader)\n",
    "    intern_loss = 0\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    for step, batch in enumerate(progress_bar, start=1):\n",
    "        \n",
    "        with amp.autocast(device_type='cuda', dtype=torch.float16):\n",
    "            anchor_emb   = get_embeddings(model_lora, attn_pool, encoder, batch['anchor_input_ids'].to(device),   \n",
    "                                          batch['anchor_attention_mask'].to(device), batch['anchor_encoded_image'].to(device))\n",
    "            positive_emb = get_embeddings(model_lora, attn_pool, encoder, batch['positive_input_ids'].to(device),\n",
    "                                          batch['positive_attention_mask'].to(device), batch['positive_encoded_image'].to(device))\n",
    "            negative_emb = get_embeddings(model_lora, attn_pool, encoder, batch['negative_input_ids'].to(device),\n",
    "                                          batch['negative_attention_mask'].to(device), batch['negative_encoded_image'].to(device))\n",
    "\n",
    "            loss = criterion(anchor_emb, positive_emb, negative_emb) / gradient_accumulation_steps\n",
    "\n",
    "        intern_loss += loss\n",
    "        scaler.scale(loss).backward()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if step % gradient_accumulation_steps == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            lr_scheduler.step()\n",
    "\n",
    "            progress_bar.set_description(\n",
    "                f\"Epoch {epoch+1} | Loss: {intern_loss.item():.5f}\"\n",
    "            )\n",
    "            intern_loss = 0\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        if (step) % (cnt // 100) == 0:\n",
    "            torch.save({\n",
    "                'encoder_state_dict': encoder.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scaler_state_dict': scaler.state_dict(),\n",
    "            }, f'encoder_{epoch + 1}_step_{step}.pt')\n",
    "\n",
    "    model_lora.eval()\n",
    "    val_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            with amp.autocast(device_type='cuda', dtype=torch.float16):\n",
    "                anchor_emb   = get_embeddings(model_lora, attn_pool, encoder, batch['anchor_input_ids'].to(device),   \n",
    "                                              batch['anchor_attention_mask'].to(device), batch['anchor_encoded_image'].to(device))\n",
    "                positive_emb = get_embeddings(model_lora, attn_pool, encoder, batch['positive_input_ids'].to(device),\n",
    "                                              batch['positive_attention_mask'].to(device), batch['positive_encoded_image'].to(device))\n",
    "                negative_emb = get_embeddings(model_lora, attn_pool, encoder, batch['negative_input_ids'].to(device),\n",
    "                                              batch['negative_attention_mask'].to(device), batch['negative_encoded_image'].to(device))\n",
    "    \n",
    "                loss = criterion(anchor_emb, positive_emb, negative_emb)\n",
    "                val_loss += loss.item()\n",
    "    \n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch + 1} completed — Avg Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a862a6-aa3f-4796-adf1-a6b6e4d2e761",
   "metadata": {},
   "source": [
    "# Extracting embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0f95f66a-fad8-4ebb-a330-e40c9c7976b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProvidedDataset(Dataset):\n",
    "    def __init__(self, unqiue_ids, tokenized_data_text, tokenized_data_img):\n",
    "        self.unqiue_ids = list(unqiue_ids)\n",
    "        self.tokenized_data_img = tokenized_data_img\n",
    "        self.tokenized_data_text = tokenized_data_text\n",
    "\n",
    "        self.keys_img = set(tokenized_data_img.keys())\n",
    "        self.keys_text = set(tokenized_data_text.keys())\n",
    "        self.text_empty_vector = torch.zeros(512)\n",
    "        self.img_empty_vector = torch.zeros(768)\n",
    "\n",
    "    def get_img(self, key):\n",
    "        return self.tokenized_data_img.get(key, self.text_empty_vector)\n",
    "\n",
    "    def get_text(self, key):\n",
    "        return self.tokenized_data_text.get(key, {\n",
    "            'input_ids': self.text_empty_vector,\n",
    "            'attention_mask': self.text_empty_vector\n",
    "        })\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        item_id = self.unqiue_ids[idx]\n",
    "        img_emb = self.get_img(item_id)\n",
    "        text_emb = self.get_text(item_id)\n",
    "        \n",
    "        return {\n",
    "            'item_id':          item_id,\n",
    "            'input_ids':        text_emb['input_ids'].squeeze(0),\n",
    "            'attention_mask':   text_emb['attention_mask'].squeeze(0),\n",
    "            'encoded_image':    img_emb.squeeze(0),\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.unqiue_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "07a50525-bc06-48f0-b29e-88aa2ed7fa75",
   "metadata": {},
   "outputs": [],
   "source": [
    "unqiue_ids = set(e for triplet in triplets for e in triplet)\n",
    "dataset = ProvidedDataset(unqiue_ids, tokenized_data_text, tokenized_data_img)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5a77f60d-8264-440c-90a3-92ebb1544ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in encoder.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8288d313-7f9c-4967-87c2-e789c7a414c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dict = {}\n",
    "\n",
    "encoder.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in dataloader:\n",
    "        with amp.autocast(device_type='cuda', dtype=torch.float16):\n",
    "            embs = get_embeddings(\n",
    "                model_lora, attn_pool, encoder,\n",
    "                batch['input_ids'].to(device),\n",
    "                batch['attention_mask'].to(device),\n",
    "                batch['encoded_image'].to(device)\n",
    "            )\n",
    "\n",
    "        for item_id, emb in zip(batch['item_id'], embs):\n",
    "            embedding_dict[item_id] = emb.cpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ad1639-3d8f-4024-a287-cc8fb0694d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bd7ef1-b110-4ac9-9e0a-c8241f014afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(embedding_dict, \"train_merged_embed.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Avito",
   "language": "python",
   "name": "avito"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
