{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5cbd9f-3091-48fd-9967-e95ac90136f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import torch\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from transformers import (\n",
    "    AutoModel,\n",
    "    AutoTokenizer,\n",
    ")\n",
    "import optuna\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity, rbf_kernel\n",
    "from sklearn.metrics import log_loss\n",
    "from catboost import CatBoostClassifier\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a63583-7dbe-4d17-bb8b-34afde825fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"avitotech_data\\\\avitotech_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3aa554b-2b25-4736-8c59-50eea1b1335a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_texts_in_dict(data: dict, tokenizer, max_length: int = 384):\n",
    "    for key, text in tqdm(data.items()):\n",
    "        if 'input_ids' in text:\n",
    "            continue\n",
    "        tokens = tokenizer(\n",
    "            text,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        tokens.pop(\"token_type_ids\", None)\n",
    "        # Убираем batch размерность (1, seq_len) -> (seq_len,)\n",
    "        data[key] = {\n",
    "            k: v.squeeze(0) for k, v in tokens.items()\n",
    "        }\n",
    "    return data\n",
    "\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output.last_hidden_state\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size())\n",
    "    sum_embeddings = (token_embeddings * input_mask_expanded).sum(1)\n",
    "    sum_mask = input_mask_expanded.sum(1)\n",
    "    return sum_embeddings / sum_mask.clamp(min=1e-9)\n",
    "\n",
    "def get_embeddings(model, input_ids, attention_mask):\n",
    "    with torch.no_grad():\n",
    "        output = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    return mean_pooling(output, attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191c320d-5184-485f-9efe-9f364e5b8574",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_average_precision(y_true, y_pred):\n",
    "    order = np.argsort(y_pred)[::-1]\n",
    "    y_true_sorted = np.array(y_true)[order]\n",
    "    n_positives = np.sum(y_true_sorted)\n",
    "    if n_positives == 0:\n",
    "        return 0.0\n",
    "\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    tp = 0\n",
    "    for k in range(1, len(y_true_sorted)+1):\n",
    "        if y_true_sorted[k-1] == 1:\n",
    "            tp += 1\n",
    "            precision = tp / k\n",
    "            recall = tp / n_positives\n",
    "            precisions.append(precision)\n",
    "            recalls.append(recall)\n",
    "    delta_recalls = [recalls[0]] + [recalls[i] - recalls[i-1] for i in range(1, len(recalls))]\n",
    "    mAP = np.sum([p * dr for p, dr in zip(precisions, delta_recalls)])\n",
    "    return mAP\n",
    "\n",
    "def objective(trial):\n",
    "    # Подбор параметров\n",
    "    params = {\n",
    "        'iterations': trial.suggest_int('iterations', 2200, 6000),\n",
    "        'depth': trial.suggest_int('depth', 7, 15),\n",
    "        'learning_rate': trial.suggest_uniform('learning_rate', 0.07, 0.25),\n",
    "        'l2_leaf_reg': trial.suggest_uniform('l2_leaf_reg', 5, 16),\n",
    "        'random_strength': trial.suggest_uniform('random_strength', 0, 2),\n",
    "        'bagging_temperature': trial.suggest_uniform('bagging_temperature', 0.1, 0.8),\n",
    "        'eval_metric': 'Logloss',\n",
    "        'loss_function': 'Logloss',\n",
    "        'task_type': 'GPU',\n",
    "        'verbose': 0\n",
    "    }\n",
    "\n",
    "    model = CatBoostClassifier(**params)\n",
    "    model.fit(\n",
    "        df_extracted_train, y_train,\n",
    "        eval_set=(df_extracted_val, y_val),\n",
    "        use_best_model=False,\n",
    "        verbose=0\n",
    "    )\n",
    "    y_val_pred_proba = model.predict_proba(df_extracted_val)[:, 1]\n",
    "    return log_loss(y_val, y_val_pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e916dff-feb0-4dd8-96da-a3c4ebfee4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = Path(\"unzipped/train\")\n",
    "\n",
    "combined_embeddings = {}\n",
    "pt_files = sorted(folder_path.glob(\"*.pt\"))\n",
    "\n",
    "folder_path_test = Path(\"unzipped/test\")\n",
    "\n",
    "combined_embeddings_test = {}\n",
    "pt_files_test = sorted(folder_path_test.glob(\"*.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e031fad-4376-4d75-8111-28c0502ec8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for pt_file in tqdm(pt_files):\n",
    "    data = torch.load(pt_file, map_location=\"cpu\")\n",
    "    combined_embeddings.update(data)\n",
    "\n",
    "for pt_file in tqdm(pt_files_test):\n",
    "    data = torch.load(pt_file, map_location=\"cpu\")\n",
    "    combined_embeddings_test.update(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79c0f97-db45-4969-b25b-9f8331bf08c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"cards_train.json\", \"r\") as file:\n",
    "    cards_train = json.load(file)\n",
    "\n",
    "with open(\"cards_test.json\", \"r\") as file:\n",
    "    cards_test = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a1cbef-830e-4aa7-826e-a8972de04884",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_1 = pd.read_parquet(\"train_part_0001.snappy.parquet\")\n",
    "df_train_2 = pd.read_parquet(\"train_part_0002.snappy.parquet\")\n",
    "df_train_3 = pd.read_parquet(\"train_part_0003.snappy.parquet\")\n",
    "df_train_4 = pd.read_parquet(\"train_part_0004.snappy.parquet\")\n",
    "\n",
    "df_test_1 = pd.read_parquet(\"test_part_0001.snappy.parquet\")\n",
    "df_test_2 = pd.read_parquet(\"test_part_0002.snappy.parquet\")\n",
    "\n",
    "df_train = pd.concat([df_train_1, df_train_2, df_train_3, df_train_4])\n",
    "df_test = pd.concat([df_test_1, df_test_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56d4f5b-a306-4873-87c6-087770988359",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.drop(['group_id', 'action_date', 'base_title',\n",
    "       'cand_title', 'base_description', 'cand_description',\n",
    "       'base_category_name', 'cand_category_name', 'base_subcategory_name',\n",
    "       'cand_subcategory_name', 'base_param1', 'cand_param1', 'base_param2',\n",
    "       'cand_param2', 'is_same_location', 'is_same_region'], axis=1)\n",
    "\n",
    "\n",
    "df_test = df_test.drop(['base_title',\n",
    "       'cand_title', 'base_description', 'cand_description',\n",
    "       'base_category_name', 'cand_category_name', 'base_subcategory_name',\n",
    "       'cand_subcategory_name', 'base_param1', 'cand_param1', 'base_param2',\n",
    "       'cand_param2', 'is_same_location', 'is_same_region'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10bf859-3f94-4006-865e-2aa3c52d93f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_data = defaultdict()\n",
    "\n",
    "for row in tqdm(df_train.iterrows(), total=len(df_train)):\n",
    "    row_element = row[1]\n",
    "    \n",
    "    base_item_id = row_element['base_item_id']\n",
    "    cand_item_id = row_element['cand_item_id']\n",
    "\n",
    "    if base_item_id not in item_data:\n",
    "        \n",
    "        item_data[base_item_id] = {\n",
    "            'price':        row_element['base_price'],\n",
    "            'json_params':  row_element['base_json_params'],\n",
    "            'count_images': row_element['base_count_images'],\n",
    "            'title_image':  row_element['base_title_image'],\n",
    "        }\n",
    "\n",
    "    if cand_item_id not in item_data:\n",
    "        \n",
    "        item_data[cand_item_id] = {\n",
    "            'price':        row_element['cand_price'],\n",
    "            'json_params':  row_element['cand_json_params'],\n",
    "            'count_images': row_element['cand_count_images'],\n",
    "            'title_image':  row_element['cand_title_image'],\n",
    "        }\n",
    "\n",
    "# test\n",
    "item_data_test = defaultdict()\n",
    "\n",
    "for row in tqdm(df_test.iterrows(), total=len(df_test)):\n",
    "    row_element = row[1]\n",
    "    \n",
    "    base_item_id = row_element['base_item_id']\n",
    "    cand_item_id = row_element['cand_item_id']\n",
    "\n",
    "    if base_item_id not in item_data_test:\n",
    "        \n",
    "        item_data_test[base_item_id] = {\n",
    "            'price':        row_element['base_price'],\n",
    "            'json_params':  row_element['base_json_params'],\n",
    "            'count_images': row_element['base_count_images'],\n",
    "            'title_image':  row_element['base_title_image'],\n",
    "        }\n",
    "\n",
    "    if cand_item_id not in item_data_test:\n",
    "        \n",
    "        item_data_test[cand_item_id] = {\n",
    "            'price':        row_element['cand_price'],\n",
    "            'json_params':  row_element['cand_json_params'],\n",
    "            'count_images': row_element['cand_count_images'],\n",
    "            'title_image':  row_element['cand_title_image'],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb3d121-6282-47dd-aa96-46e481ad1acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item_id, values in tqdm(item_data.items(), total=len(item_data)):\n",
    "    img_name = values['title_image'] + '.jpg'\n",
    "    if img_name in combined_embeddings:\n",
    "        values['image_embed'] = combined_embeddings[img_name]\n",
    "    else:\n",
    "        values['image_embed'] = torch.zeros(768)\n",
    "\n",
    "# test\n",
    "for item_id, values in tqdm(item_data_test.items(), total=len(item_data_test)):\n",
    "    img_name = values['title_image'] + '.jpg'\n",
    "    if img_name in combined_embeddings_test:\n",
    "        values['image_embed'] = combined_embeddings_test[img_name]\n",
    "    else:\n",
    "        values['image_embed'] = torch.zeros(768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a5e4a5-0497-466f-95be-aa9864569124",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_name = \"cointegrated/rubert-tiny2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da577e79-5648-4df9-a20c-855592424c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data_text = tokenize_texts_in_dict(cards_train, tokenizer)\n",
    "tokenized_data_text_test = tokenize_texts_in_dict(cards_test, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9fe4b5-f053-4516-8f1c-927e42804171",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item_id, values in tqdm(item_data.items(), total=len(item_data)):\n",
    "    input_ids = tokenized_data_text[item_id]['input_ids'].unsqueeze(0).to(device)\n",
    "    attention_mask = tokenized_data_text[item_id]['attention_mask'].unsqueeze(0).to(device)\n",
    "    values['text_embed'] = get_embeddings(model, input_ids=input_ids, attention_mask=attention_mask).squeeze(0)\n",
    "\n",
    "# test\n",
    "for item_id, values in tqdm(item_data_test.items(), total=len(item_data_test)):\n",
    "    input_ids = tokenized_data_text_test[item_id]['input_ids'].unsqueeze(0).to(device)\n",
    "    attention_mask = tokenized_data_text_test[item_id]['attention_mask'].unsqueeze(0).to(device)\n",
    "    values['text_embed'] = get_embeddings(model, input_ids=input_ids, attention_mask=attention_mask).squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0445b6-3d6c-4851-a0c7-8067a2b0601f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y= []\n",
    "\n",
    "for row in tqdm(df_train.iterrows(), total=len(df_train)):\n",
    "    row_element = row[1]\n",
    "    \n",
    "    base_item_id = row_element['base_item_id']\n",
    "    cand_item_id = row_element['cand_item_id']\n",
    "\n",
    "    if base_item_id in item_data.keys() and cand_item_id in item_data.keys():\n",
    "        base_price = item_data[base_item_id]['price']\n",
    "        cand_price = item_data[cand_item_id]['price']\n",
    "\n",
    "        base_json_params = json.loads(item_data[base_item_id]['json_params'])\n",
    "        cand_json_params = json.loads(item_data[cand_item_id]['json_params'])\n",
    "\n",
    "        base_count_images = item_data[base_item_id]['count_images']\n",
    "        cand_count_images = item_data[cand_item_id]['count_images']\n",
    "\n",
    "        base_img_embed = item_data[base_item_id]['image_embed'].reshape(1, -1)\n",
    "        cand_img_embed = item_data[cand_item_id]['image_embed'].reshape(1, -1)\n",
    "\n",
    "        base_text_embed = item_data[base_item_id]['text_embed'].reshape(1, -1).cpu()\n",
    "        cand_text_embed = item_data[cand_item_id]['text_embed'].reshape(1, -1).cpu()\n",
    "\n",
    "        is_double = row_element['is_double']\n",
    "        # price\n",
    "        price_dif = 2 * abs(base_price - cand_price) / max((base_price + cand_price), 1)\n",
    "\n",
    "        # json\n",
    "        base_unique_keys = set(base_json_params.keys())\n",
    "        cand_unique_keys = set(cand_json_params.keys())\n",
    "\n",
    "        intersect = base_unique_keys.intersection(cand_unique_keys)\n",
    "        union = base_unique_keys.union(cand_unique_keys)\n",
    "\n",
    "        ## a. Jaccard\n",
    "        jaccard = 1 if len(union) == 0 else len(intersect) / len(union)\n",
    "\n",
    "        ## b. Ratio of intersect\n",
    "        ratio = 1 if len(union) == 0 else len(intersect) / max(min(len(base_unique_keys), len(cand_unique_keys)), 1)\n",
    "\n",
    "        ## c. shared\n",
    "        shared_int = 0\n",
    "        shared_float = 0\n",
    "        shared_str = 0\n",
    "        shared_list = 0\n",
    "        \n",
    "        intersect_int = 0\n",
    "        intersect_float = 0\n",
    "        intersect_str = 0\n",
    "        intersect_list = 0\n",
    "\n",
    "        for unique_key in intersect:\n",
    "            # int\n",
    "            if isinstance(base_json_params[unique_key], int) and isinstance(cand_json_params[unique_key], int):\n",
    "                intersect_int += 1\n",
    "                if base_json_params[unique_key] == cand_json_params[unique_key]:\n",
    "                    shared_int += 1\n",
    "\n",
    "            # float\n",
    "            if isinstance(base_json_params[unique_key], float) and isinstance(cand_json_params[unique_key], float):\n",
    "                intersect_float += 1\n",
    "                if base_json_params[unique_key] == cand_json_params[unique_key]:\n",
    "                    shared_float += 1\n",
    "\n",
    "            # str\n",
    "            if isinstance(base_json_params[unique_key], str) and isinstance(cand_json_params[unique_key], str):\n",
    "                intersect_str += 1\n",
    "                if base_json_params[unique_key] == cand_json_params[unique_key]:\n",
    "                    shared_str += 1\n",
    "\n",
    "            # list\n",
    "            if isinstance(base_json_params[unique_key], list) and isinstance(cand_json_params[unique_key], list):\n",
    "                intersect_list += 1\n",
    "                if len(base_json_params[unique_key]) == 0 or len(cand_json_params[unique_key]) == 0:\n",
    "                    continue\n",
    "                \n",
    "                if isinstance(base_json_params[unique_key][0], dict) or isinstance(cand_json_params[unique_key][0], dict):\n",
    "                    if set(base_json_params[unique_key][0].keys()) == set(cand_json_params[unique_key][0].keys()):\n",
    "                        shared_list += 1\n",
    "                elif set(base_json_params[unique_key]) == set(cand_json_params[unique_key]):\n",
    "                    shared_list += 1\n",
    "\n",
    "        shared = shared_int + shared_float + shared_str + shared_list\n",
    "\n",
    "        same_items_ratio       = shared / max(len(intersect), 1)\n",
    "        same_items_ratio_int   = shared_int / max(intersect_int, 1)\n",
    "        same_items_ratio_float = shared_float / max(intersect_float, 1)\n",
    "        same_items_ratio_str   = shared_str / max(intersect_str, 1)\n",
    "        same_items_ratio_list  = shared_list / max(intersect_list, 1)\n",
    "\n",
    "        # jaccard per type\n",
    "        union_int = set()\n",
    "        union_float = set()\n",
    "        union_str = set()\n",
    "        union_list = set()\n",
    "\n",
    "        for key, value in base_json_params.items():\n",
    "            if isinstance(value, int):\n",
    "                union_int.add(key)\n",
    "            elif isinstance(value, float):\n",
    "                union_float.add(key)\n",
    "            elif isinstance(value, str):\n",
    "                union_str.add(key)\n",
    "            elif isinstance(value, list):\n",
    "                union_list.add(key)\n",
    "\n",
    "        for key, value in cand_json_params.items():\n",
    "            if isinstance(value, int):\n",
    "                union_int.add(key)\n",
    "            elif isinstance(value, float):\n",
    "                union_float.add(key)\n",
    "            elif isinstance(value, str):\n",
    "                union_str.add(key)\n",
    "            elif isinstance(value, list):\n",
    "                union_list.add(key)\n",
    "\n",
    "        jaccard_int = 1 if len(union_int) == 0 else intersect_int / len(union_int)\n",
    "        jaccard_float = 1 if len(union_float) == 0 else intersect_float / len(union_float)\n",
    "        jaccard_str = 1 if len(union_str) == 0 else intersect_str / len(union_str)\n",
    "        jaccard_list = 1 if len(union_list) == 0 else intersect_list / len(union_list)\n",
    "\n",
    "        img_diff = abs(0 if math.isnan(base_count_images) else base_count_images - 0 if math.isnan(cand_count_images) else cand_count_images)\n",
    "\n",
    "        # image\n",
    "        ## cosine_similarity\n",
    "        cos_sim_img = cosine_similarity(base_img_embed, cand_img_embed).item()\n",
    "\n",
    "        ## rbf kernel\n",
    "        rbf_img = rbf_kernel(base_img_embed, cand_img_embed).item()\n",
    "\n",
    "        # text\n",
    "        ## cosine_similarity\n",
    "        cos_sim_text = cosine_similarity(base_text_embed, cand_text_embed).item()\n",
    "\n",
    "        ## rbf kernel\n",
    "        rbf_text = rbf_kernel(base_text_embed, cand_text_embed).item()\n",
    "\n",
    "        X.append(\n",
    "            {\n",
    "                'price_dif': round(price_dif),\n",
    "                'jaccard': round(jaccard, 5),\n",
    "                'jaccard_int': round(jaccard_int, 5),\n",
    "                'jaccard_float': round(jaccard_float, 5),\n",
    "                'jaccard_str': round(jaccard_str, 5),\n",
    "                'jaccard_list': round(jaccard_list, 5),\n",
    "                'ratio': round(ratio, 5),\n",
    "                'same_items_ratio': round(same_items_ratio, 5),\n",
    "                'same_items_ratio_int': round(same_items_ratio_int, 5),\n",
    "                'same_items_ratio_float': round(same_items_ratio_float, 5),\n",
    "                'same_items_ratio_str': round(same_items_ratio_str, 5),\n",
    "                'same_items_ratio_list': round(same_items_ratio_list, 5),\n",
    "                'img_diff': round(img_diff),\n",
    "                'cos_sim_img': round(cos_sim_img, 5),\n",
    "                'rbf_img': round(rbf_img, 5),\n",
    "                'cos_sim_text': round(cos_sim_text, 5),\n",
    "                'rbf_text': round(rbf_text, 5)\n",
    "            }\n",
    "        )\n",
    "\n",
    "        y.append(is_double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a84548-1824-4c7e-9a25-e213fc117542",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = []\n",
    "\n",
    "for row in tqdm(df_test.iterrows(), total=len(df_test)):\n",
    "    row_element = row[1]\n",
    "    \n",
    "    base_item_id = row_element['base_item_id']\n",
    "    cand_item_id = row_element['cand_item_id']\n",
    "\n",
    "    if base_item_id in item_data_test.keys() and cand_item_id in item_data_test.keys():\n",
    "        base_price = item_data_test[base_item_id]['price']\n",
    "        cand_price = item_data_test[cand_item_id]['price']\n",
    "\n",
    "        base_json_params = json.loads(item_data_test[base_item_id]['json_params'])\n",
    "        cand_json_params = json.loads(item_data_test[cand_item_id]['json_params'])\n",
    "\n",
    "        base_count_images = item_data_test[base_item_id]['count_images']\n",
    "        cand_count_images = item_data_test[cand_item_id]['count_images']\n",
    "\n",
    "        base_img_embed = item_data_test[base_item_id]['image_embed'].reshape(1, -1)\n",
    "        cand_img_embed = item_data_test[cand_item_id]['image_embed'].reshape(1, -1)\n",
    "\n",
    "        base_text_embed = item_data_test[base_item_id]['text_embed'].reshape(1, -1).cpu()\n",
    "        cand_text_embed = item_data_test[cand_item_id]['text_embed'].reshape(1, -1).cpu()\n",
    "\n",
    "        # price\n",
    "        price_dif = 2 * abs(base_price - cand_price) / max((base_price + cand_price), 1)\n",
    "\n",
    "        # json\n",
    "        base_unique_keys = set(base_json_params.keys())\n",
    "        cand_unique_keys = set(cand_json_params.keys())\n",
    "\n",
    "        intersect = base_unique_keys.intersection(cand_unique_keys)\n",
    "        union = base_unique_keys.union(cand_unique_keys)\n",
    "\n",
    "        ## a. Jaccard\n",
    "        jaccard = 1 if len(union) == 0 else len(intersect) / len(union)\n",
    "\n",
    "        ## b. Ratio of intersect\n",
    "        ratio = 1 if len(union) == 0 else len(intersect) / max(min(len(base_unique_keys), len(cand_unique_keys)), 1)\n",
    "\n",
    "        ## c. shared\n",
    "        shared_int = 0\n",
    "        shared_float = 0\n",
    "        shared_str = 0\n",
    "        shared_list = 0\n",
    "        \n",
    "        intersect_int = 0\n",
    "        intersect_float = 0\n",
    "        intersect_str = 0\n",
    "        intersect_list = 0\n",
    "\n",
    "        for unique_key in intersect:\n",
    "            # int\n",
    "            if isinstance(base_json_params[unique_key], int) and isinstance(cand_json_params[unique_key], int):\n",
    "                intersect_int += 1\n",
    "                if base_json_params[unique_key] == cand_json_params[unique_key]:\n",
    "                    shared_int += 1\n",
    "\n",
    "            # float\n",
    "            if isinstance(base_json_params[unique_key], float) and isinstance(cand_json_params[unique_key], float):\n",
    "                intersect_float += 1\n",
    "                if base_json_params[unique_key] == cand_json_params[unique_key]:\n",
    "                    shared_float += 1\n",
    "\n",
    "            # str\n",
    "            if isinstance(base_json_params[unique_key], str) and isinstance(cand_json_params[unique_key], str):\n",
    "                intersect_str += 1\n",
    "                if base_json_params[unique_key] == cand_json_params[unique_key]:\n",
    "                    shared_str += 1\n",
    "\n",
    "            # list\n",
    "            if isinstance(base_json_params[unique_key], list) and isinstance(cand_json_params[unique_key], list):\n",
    "                intersect_list += 1\n",
    "                if len(base_json_params[unique_key]) == 0 or len(cand_json_params[unique_key]) == 0:\n",
    "                    continue\n",
    "                \n",
    "                if isinstance(base_json_params[unique_key][0], dict) or isinstance(cand_json_params[unique_key][0], dict):\n",
    "                    if set(base_json_params[unique_key][0].keys()) == set(cand_json_params[unique_key][0].keys()):\n",
    "                        shared_list += 1\n",
    "                elif set(base_json_params[unique_key]) == set(cand_json_params[unique_key]):\n",
    "                    shared_list += 1\n",
    "\n",
    "        shared = shared_int + shared_float + shared_str + shared_list\n",
    "\n",
    "        same_items_ratio       = shared / max(len(intersect), 1)\n",
    "        same_items_ratio_int   = shared_int / max(intersect_int, 1)\n",
    "        same_items_ratio_float = shared_float / max(intersect_float, 1)\n",
    "        same_items_ratio_str   = shared_str / max(intersect_str, 1)\n",
    "        same_items_ratio_list  = shared_list / max(intersect_list, 1)\n",
    "\n",
    "        # jaccard per type\n",
    "        union_int = set()\n",
    "        union_float = set()\n",
    "        union_str = set()\n",
    "        union_list = set()\n",
    "\n",
    "        for key, value in base_json_params.items():\n",
    "            if isinstance(value, int):\n",
    "                union_int.add(key)\n",
    "            elif isinstance(value, float):\n",
    "                union_float.add(key)\n",
    "            elif isinstance(value, str):\n",
    "                union_str.add(key)\n",
    "            elif isinstance(value, list):\n",
    "                union_list.add(key)\n",
    "\n",
    "        for key, value in cand_json_params.items():\n",
    "            if isinstance(value, int):\n",
    "                union_int.add(key)\n",
    "            elif isinstance(value, float):\n",
    "                union_float.add(key)\n",
    "            elif isinstance(value, str):\n",
    "                union_str.add(key)\n",
    "            elif isinstance(value, list):\n",
    "                union_list.add(key)\n",
    "\n",
    "        jaccard_int = 1 if len(union_int) == 0 else intersect_int / len(union_int)\n",
    "        jaccard_float = 1 if len(union_float) == 0 else intersect_float / len(union_float)\n",
    "        jaccard_str = 1 if len(union_str) == 0 else intersect_str / len(union_str)\n",
    "        jaccard_list = 1 if len(union_list) == 0 else intersect_list / len(union_list)\n",
    "\n",
    "        img_diff = abs(0 if math.isnan(base_count_images) else base_count_images - 0 if math.isnan(cand_count_images) else cand_count_images)\n",
    "\n",
    "        # image\n",
    "        ## cosine_similarity\n",
    "        cos_sim_img = cosine_similarity(base_img_embed, cand_img_embed).item()\n",
    "\n",
    "        ## rbf kernel\n",
    "        rbf_img = rbf_kernel(base_img_embed, cand_img_embed).item()\n",
    "\n",
    "        # text\n",
    "        ## cosine_similarity\n",
    "        cos_sim_text = cosine_similarity(base_text_embed, cand_text_embed).item()\n",
    "\n",
    "        ## rbf kernel\n",
    "        rbf_text = rbf_kernel(base_text_embed, cand_text_embed).item()\n",
    "\n",
    "        X_test.append(\n",
    "            {\n",
    "                'price_dif': round(price_dif),\n",
    "                'jaccard': round(jaccard, 5),\n",
    "                'jaccard_int': round(jaccard_int, 5),\n",
    "                'jaccard_float': round(jaccard_float, 5),\n",
    "                'jaccard_str': round(jaccard_str, 5),\n",
    "                'jaccard_list': round(jaccard_list, 5),\n",
    "                'ratio': round(ratio, 5),\n",
    "                'same_items_ratio': round(same_items_ratio, 5),\n",
    "                'same_items_ratio_int': round(same_items_ratio_int, 5),\n",
    "                'same_items_ratio_float': round(same_items_ratio_float, 5),\n",
    "                'same_items_ratio_str': round(same_items_ratio_str, 5),\n",
    "                'same_items_ratio_list': round(same_items_ratio_list, 5),\n",
    "                'img_diff': round(img_diff),\n",
    "                'cos_sim_img': round(cos_sim_img, 5),\n",
    "                'rbf_img': round(rbf_img, 5),\n",
    "                'cos_sim_text': round(cos_sim_text, 5),\n",
    "                'rbf_text': round(rbf_text, 5)\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2877fe5-d87a-4536-9960-02c28989f47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde09676-600f-4fd6-9891-39b086a43672",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_extracted_train = pd.DataFrame(X_train)\n",
    "df_extracted_val = pd.DataFrame(X_val)\n",
    "\n",
    "df_extracted_test = pd.DataFrame(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca971bb6-5c6f-4fbb-bc06-557b62f67d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = pd.DataFrame(y_train)\n",
    "y_val = pd.DataFrame(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d82608-2b9b-4bda-9fe1-350da3b8fa7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_extracted_train.to_csv('df_extracted_train.csv', index=False)\n",
    "df_extracted_val.to_csv('df_extracted_val.csv', index=False)\n",
    "df_extracted_test.to_csv('df_extracted_test.csv', index=False)\n",
    "\n",
    "y_train.to_csv('y_train.csv', index=False)\n",
    "y_val.to_csv('y_val.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a604c7b3-042e-4090-9fa6-cc4dd717a83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_extracted_train = pd.read_csv('df_extracted_train.csv')\n",
    "df_extracted_val = pd.read_csv('df_extracted_val.csv')\n",
    "df_extracted_test = pd.read_csv('df_extracted_test.csv')\n",
    "\n",
    "y_train = pd.read_csv('y_train.csv')\n",
    "y_val = pd.read_csv('y_val.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9f1847-43c2-40ed-b6ad-5b43d164e316",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=150)\n",
    "\n",
    "print(\"Best params:\", study.best_params)\n",
    "print(\"Best mAP:\", study.best_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9731207-26b9-4ab7-89f1-3ac4412df5a2",
   "metadata": {},
   "source": [
    "## Discovery the best range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeeed3da-ef5a-46a6-bdbd-c927a361e01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = []\n",
    "depth = []\n",
    "learning_rate =[]\n",
    "l2_leaf_reg= []\n",
    "random_strength =[]\n",
    "bagging_temperature = []\n",
    "metric = []\n",
    "\n",
    "for idx, t in enumerate(study.trials):\n",
    "    if idx == 60:\n",
    "        break\n",
    "    iterations.append(t.params['iterations'])\n",
    "    depth.append(t.params['depth'])\n",
    "    learning_rate.append(t.params['learning_rate'])\n",
    "    l2_leaf_reg.append(t.params['l2_leaf_reg'])\n",
    "    random_strength.append(t.params['random_strength'])\n",
    "    bagging_temperature.append(t.params['bagging_temperature'])\n",
    "    metric.append(t.values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c06993c-82d7-48ad-ab4b-ce2e9e8aa020",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\n",
    "    'iterations',\n",
    "    'depth',\n",
    "    'learning_rate',\n",
    "    'l2_leaf_reg',\n",
    "    'random_strength',\n",
    "    'bagging_temperature'\n",
    "]\n",
    "\n",
    "steps = {\n",
    "    'iterations':          100,\n",
    "    'depth':               1,\n",
    "    'learning_rate':       0.01,\n",
    "    'l2_leaf_reg':         1,\n",
    "    'random_strength':     0.1,\n",
    "    'bagging_temperature': 0.1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abaaa29-6383-444c-b623-ce394df1ff37",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_array = np.array(metric)\n",
    "metric_norm = (metric_array - metric_array.min()) / (metric_array.max() - metric_array.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b334c2-b4cd-4bcb-ab70-e30b55cb8ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_correction = pd.DataFrame({'iterations': iterations,\n",
    "                              'depth': depth,\n",
    "                              'learning_rate': learning_rate,\n",
    "                              'l2_leaf_reg': l2_leaf_reg,\n",
    "                              'random_strength': random_strength,\n",
    "                              'bagging_temperature': bagging_temperature,\n",
    "                              'metric_norm': metric_norm})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a029ca7-d4fa-496c-83da-68c4954e00d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins_dict = {}\n",
    "labels_dict = {}\n",
    "\n",
    "for col in cols:\n",
    "    mn = df_correction[col].min()\n",
    "    mx = df_correction[col].max()\n",
    "    step = steps[col]\n",
    "    # создаём бины\n",
    "    bins = np.arange(mn, mx + step, step)\n",
    "    bins_dict[col] = bins\n",
    "    # выбираем формат меток\n",
    "    fmt = \".2f\" if step < 1 else \".0f\"\n",
    "    # создаём метки\n",
    "    labels = [f\"{bins[i]:{fmt}}-{bins[i+1]:{fmt}}\" for i in range(len(bins)-1)]\n",
    "    labels_dict[col] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1295afc3-93cb-4a90-9f9a-375135b0c5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in cols:\n",
    "    df_correction[col] = pd.cut(\n",
    "        df_correction[col],\n",
    "        bins=bins_dict[col],\n",
    "        labels=labels_dict[col],\n",
    "        include_lowest=True,\n",
    "        right=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be1a15a-02ac-405b-b7a8-96b668822acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in cols:\n",
    "    agg = (\n",
    "        df_correction\n",
    "        .groupby(col, observed=False)['metric_norm']\n",
    "        .sum()\n",
    "        .reset_index()\n",
    "    )\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.bar(agg[col].astype(str), agg['metric_norm'])\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.title(f'Sum of metric_norm by {col}')\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Sum of metric_norm')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29fec63-d835-4cc0-ac2a-5a066b5027b2",
   "metadata": {},
   "source": [
    "# The training of the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d45e10-37b3-4bb7-b8ee-d5174d631397",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = study.best_params\n",
    "best_model = CatBoostClassifier(**best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0085ae0-68d9-4a3d-a7a4-233f9de1dbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.fit(\n",
    "    pd.concat([df_extracted_train, df_extracted_val]),\n",
    "    pd.concat([y_train, y_val]),\n",
    "    verbose=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da273327-8ea4-4bb7-ad83-ba484e133f56",
   "metadata": {},
   "source": [
    "# Predict params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafb9451-133b-4d02-a071-f49f7ae2d77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test.drop(['base_price', 'cand_price',\n",
    "       'base_json_params', 'cand_json_params', 'base_count_images',\n",
    "       'cand_count_images', 'base_title_image', 'cand_title_image'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae062d7-3498-4671-bbc3-c5dc286e9190",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_proba = best_model.predict_proba(df_extracted_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdff411-e782-470a-8a59-bcfd4387e0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_proba = y_test_proba.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5029a29-a2c8-45a1-8f5e-84860915e2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[\"y_test_proba\"] = y_test_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2270d16-60a5-4e17-97d6-46f3bdace783",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test.rename(columns={\n",
    "    df_test.columns[0]: \"base_id\",\n",
    "    df_test.columns[1]: \"cand_id\",\n",
    "    df_test.columns[2]: \"probability\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5219b7-500e-483b-8393-448a57c57dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.to_csv(\"submission.csv\", index=False, sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b728977-c81a-4482-9c21-0277f78bd375",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Avito",
   "language": "python",
   "name": "avito"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
