{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe477fb-291e-49a3-b003-39042a188a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import amp\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoModel,\n",
    "    AutoTokenizer,\n",
    "    get_scheduler,\n",
    ")\n",
    "from peft import get_peft_model, LoraConfig\n",
    "import json\n",
    "import glob\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f585554-ad0a-4612-907f-c78ecfe9bf69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_triplets(data):\n",
    "    triplets = []\n",
    "\n",
    "    for anchor, info in data.items():\n",
    "        pair = info['pair']\n",
    "        is_double = info['is_double']\n",
    "\n",
    "        positives = [p for p, d in zip(pair, is_double) if d == 1]\n",
    "        negatives = [p for p, d in zip(pair, is_double) if d == 0]\n",
    "\n",
    "        assert len(negatives) >= 6 * len(positives), f\"Not enough negatives for anchor {anchor}\"\n",
    "\n",
    "        for i, pos in enumerate(positives):\n",
    "            start = i * 6\n",
    "            end = start + 6\n",
    "            negs_for_pos = negatives[start:end]\n",
    "            for neg in negs_for_pos:\n",
    "                triplets.append((anchor, pos, neg))\n",
    "\n",
    "    return triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0aa95e8-0789-4fa0-8eec-b0e93203d083",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletDataset(Dataset):\n",
    "    def __init__(self, triplets, tokenized_data):\n",
    "        self.triplets = triplets\n",
    "        self.tokenized_data = tokenized_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.triplets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        a, p, n = self.triplets[idx]\n",
    "        da = self.tokenized_data[a]\n",
    "        dp = self.tokenized_data[p]\n",
    "        dn = self.tokenized_data[n]\n",
    "        return {\n",
    "            'anchor_input_ids':    da['input_ids'].squeeze(0),\n",
    "            'anchor_attention_mask': da['attention_mask'].squeeze(0),\n",
    "            'positive_input_ids':  dp['input_ids'].squeeze(0),\n",
    "            'positive_attention_mask': dp['attention_mask'].squeeze(0),\n",
    "            'negative_input_ids':  dn['input_ids'].squeeze(0),\n",
    "            'negative_attention_mask': dn['attention_mask'].squeeze(0),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b241ab6a-7b44-4d0b-b998-baf21e248ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_numbers(filename):\n",
    "    # Получаем числа из строки\n",
    "    parts = filename.replace(\".pt\", \"\").split(\"_\")\n",
    "    return int(parts[2]), int(parts[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4689661d-0357-4c5c-818d-25eda44fc3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_texts_in_dict(data: dict, tokenizer, max_length: int = 384):\n",
    "    for key, text in tqdm(data.items()):\n",
    "        if 'input_ids' in text:\n",
    "            continue\n",
    "        tokens = tokenizer(\n",
    "            text,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        tokens.pop(\"token_type_ids\", None)\n",
    "        # Убираем batch размерность (1, seq_len) -> (seq_len,)\n",
    "        data[key] = {\n",
    "            k: v.squeeze(0) for k, v in tokens.items()\n",
    "        }\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44903c42-5a4b-48ce-ac71-9d8d4ef5f817",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnPooling(nn.Module):\n",
    "    \"\"\"\n",
    "    Learnable attention pooling: весит каждый токен по-разному.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size: int = 312):\n",
    "        super().__init__()\n",
    "        # \"Запрос\" q, который обучается вместе с остальной сеткой\n",
    "        self.q = nn.Parameter(torch.randn(hidden_size))\n",
    "\n",
    "    def forward(self,\n",
    "                hidden_states: torch.Tensor,\n",
    "                attention_mask: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        hidden_states : (B, T, 312)\n",
    "        attention_mask: (B, T)\n",
    "        \"\"\"\n",
    "        # (B,T) — скалярное произведение q и каждого токена\n",
    "        scores = (hidden_states @ self.q) / hidden_states.size(-1) ** 0.5\n",
    "        scores = scores.masked_fill(attention_mask == 0, -1e4)  # exclude pads\n",
    "        weights = F.softmax(scores, dim=1).unsqueeze(-1)        # (B,T,1)\n",
    "\n",
    "        pooled = (weights * hidden_states).sum(dim=1)           # (B,312)\n",
    "        return pooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16e83f4-6aab-416b-9ffc-bb667f251bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(model, attn_pool, input_ids, attention_mask):\n",
    "    out = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    pooled = attn_pool(out.last_hidden_state.float(), attention_mask) # (B, 312)\n",
    "    return pooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6dc9c4-0114-4127-a2da-85c47865ad90",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('avitotech_data/avitotech_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07afc97-2a53-4ec7-8609-f279f521d541",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('to_undergo.json', 'r') as f:\n",
    "    ids_trunc = json.load(f)\n",
    "\n",
    "with open(\"cards_train.json\", \"r\") as file:\n",
    "    cards_train = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bd8f1f-4209-4087-8a27-000b2c4f690d",
   "metadata": {},
   "outputs": [],
   "source": [
    "triplets = build_triplets(ids_trunc)\n",
    "train_triplets, val_triplets = train_test_split(triplets, test_size=0.1, random_state=42)\n",
    "\n",
    "tokenized_data_text = tokenize_texts_in_dict(cards_train, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf91b63-46cf-4e4d-9afb-57127e1be05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_name = \"cointegrated/rubert-tiny2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,  # размер low-rank матриц A и B\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"query\", \"value\"],  # модули attention, которые дообучаем\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"FEATURE_EXTRACTION\"\n",
    ")\n",
    "\n",
    "model_lora = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfe82ff-e814-4abe-a17b-73fab2b9f7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TripletDataset(train_triplets, tokenized_data_text)\n",
    "val_dataset   = TripletDataset(val_triplets, tokenized_data_text)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "attn_pool = AttnPooling(312)\n",
    "attn_pool = attn_pool.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16921589-0ce1-4d78-97d1-9c3996d2d979",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(\n",
    "    list(attn_pool.parameters()) +\n",
    "    list(model_lora.parameters()),\n",
    "    lr=2e-5\n",
    ")\n",
    "criterion = nn.TripletMarginLoss(margin=2, p=2)\n",
    "\n",
    "epochs = 3\n",
    "gradient_accumulation_steps = 5\n",
    "num_training_steps = epochs * len(train_loader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "scaler = amp.GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f75c8e-192d-458f-b390-28bcae7b19d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"trained_text_models\", exist_ok=True)\n",
    "\n",
    "os.chdir(\"trained_text_models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7afbfc-2d34-4b6a-bf9c-3f3ee7f63361",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = glob.glob('lora_triplet_*_step_*.pt')\n",
    "\n",
    "if models:\n",
    "    last_chpt = max(models, key=extract_numbers)\n",
    "    checkpoint = torch.load(last_chpt, map_location=device)\n",
    "    \n",
    "    model_lora.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    scaler.load_state_dict(checkpoint['scaler_state_dict'])\n",
    "model_lora = model_lora.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9587d399-b705-4d9a-a40a-f649adac70d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "    model_lora.train()\n",
    "    total_loss = 0.0\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch}\")\n",
    "\n",
    "    cnt = len(train_loader)\n",
    "    intern_loss = 0\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    for step, batch in enumerate(progress_bar, start=1):\n",
    "        \n",
    "        with amp.autocast(device_type='cuda', dtype=torch.float16):\n",
    "            anchor_emb   = get_embeddings(model_lora, attn_pool, batch['anchor_input_ids'].to(device),   batch['anchor_attention_mask'].to(device))\n",
    "            positive_emb = get_embeddings(model_lora, attn_pool, batch['positive_input_ids'].to(device), batch['positive_attention_mask'].to(device))\n",
    "            negative_emb = get_embeddings(model_lora, attn_pool, batch['negative_input_ids'].to(device), batch['negative_attention_mask'].to(device))\n",
    "\n",
    "            loss = criterion(anchor_emb, positive_emb, negative_emb) / gradient_accumulation_steps\n",
    "\n",
    "        intern_loss += loss\n",
    "        scaler.scale(loss).backward()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if step % gradient_accumulation_steps == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            lr_scheduler.step()\n",
    "\n",
    "            progress_bar.set_description(\n",
    "                f\"Epoch {epoch+1} | Loss: {intern_loss.item():.5f}\"\n",
    "            )\n",
    "            intern_loss = 0\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        if (step) % (cnt // 100) == 0:\n",
    "            torch.save({\n",
    "                'model_state_dict': model_lora.state_dict(),\n",
    "                'attn_state_dict': attn_pool.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scaler_state_dict': scaler.state_dict(),\n",
    "            }, f'lora_triplet_{epoch + 1}_step_{step}.pt')\n",
    "\n",
    "    model_lora.eval()\n",
    "    val_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            with amp.autocast(device_type='cuda', dtype=torch.float16):\n",
    "                anchor_emb   = get_embeddings(model_lora, attn_pool, batch['anchor_input_ids'].to(device), batch['anchor_attention_mask'].to(device))\n",
    "                positive_emb = get_embeddings(model_lora, attn_pool, batch['positive_input_ids'].to(device), batch['positive_attention_mask'].to(device))\n",
    "                negative_emb = get_embeddings(model_lora, attn_pool, batch['negative_input_ids'].to(device), batch['negative_attention_mask'].to(device))\n",
    "    \n",
    "                loss = criterion(anchor_emb, positive_emb, negative_emb)\n",
    "                val_loss += loss.item()\n",
    "    \n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Epoch {epoch + 1} completed — Avg Loss: {avg_loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Avito",
   "language": "python",
   "name": "avito"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
